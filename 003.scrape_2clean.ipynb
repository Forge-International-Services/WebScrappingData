{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from docx import Document\n",
    "import re\n",
    "import glob\n",
    "import openpyxl\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import win32com.client as win32\n",
    "import logging\n",
    "from tld import get_fld\n",
    "import pypandoc\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin, unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for downloads\n",
    "DOWNLOAD_DIR = \"999_downloaded_documents_link1\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from file\n",
    "def read_urls(file_name='999.web_urls.txt'):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Función auxiliar para verificar si un enlace es interno (pertenece al mismo dominio)\n",
    "# Esta funcion es un componente del proceso en bloque 7 que \"scrape and store document links in JSON with parent page path segments\" ---> esta funcion evita enviar request a links por fuera del dominio al que estamos scrapping\n",
    "\n",
    "\n",
    "# Helper function to check if a link is internal (within the same domain)\n",
    "def is_internal_link(url, base_url):\n",
    "    # Parse both URLs\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "\n",
    "    # Check if the domain of the URL matches the base URL's domain\n",
    "    # TRUE si el dominio es el mismo que el base pero FULL URL, RELATIVE PATH, SAME DOMAIN BUT DIFFERENT PROTOCOL else FALSE\n",
    "    return parsed_url.netloc == parsed_base_url.netloc or parsed_url.netloc == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si el contenido de una respuesta es un formato binario (como PDF o Word)\n",
    "#'application/pdf' for PDFs,'application/msword' for Word documents, and 'application/vnd' for various document formats (e.g., Excel or OpenDocument).\n",
    "# THIS FUNCTION IS A COMPONENT FOR get_document_links() Function\n",
    "\n",
    "def is_binary_content(response):\n",
    "    content_type = response.headers.get('Content-Type', '').lower()\n",
    "    return 'application/pdf' in content_type or 'application/msword' in content_type or 'application/vnd' in content_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar nombres de archivos extraídos de URLs\n",
    "# The cleaned file_name is returned, making it suitable for use as a filename.\n",
    "# THIS FUNCTION IS A COMPONENT FOR download_file() Function\n",
    "\n",
    "# Function to clean filenames\n",
    "def clean_filename(href):\n",
    "    # Extract the file name from the URL\n",
    "    file_name = unquote(os.path.basename(urlparse(href).path))\n",
    "    # Remove invalid characters for filenames using a regex\n",
    "    file_name = re.sub(r'[<>:\"/\\\\|?*]', '_', file_name)\n",
    "    # Fallback if the URL doesn't have a file name\n",
    "    if not file_name:\n",
    "        file_name = \"downloaded_file\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener enlaces de documentos desde un sitio web, navegando recursivamente por enlaces internos.\n",
    "# THIS FUNCTION IS A COMPONEN FOR scrape_and_store_links() and scrape_documents_from_website() FUNCTIONS\n",
    "\n",
    "# Recursively fetch document links from a website, traversing internal links.\n",
    "def get_document_links(url, base_url, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    doc_links = []\n",
    "\n",
    "    # Ensure we do not revisit the same page more than once\n",
    "    if url in visited:\n",
    "        return doc_links\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.write(f\"Processing: {url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Skip binary files (like PDFs, DOCs) to avoid trying to parse them\n",
    "        if is_binary_content(response):\n",
    "            sys.stdout.write(f\"Skipping binary file: {url}\\r\")\n",
    "            sys.stdout.flush()\n",
    "            return doc_links\n",
    "\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all document links on the page (pdf, doc, docx)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith(('tel:', 'mailto:')):\n",
    "                continue\n",
    "            if href.endswith(('pdf', 'doc', 'docx','doc')):\n",
    "                full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "                # Append both the parent page (url) and the document link (full_url)\n",
    "                doc_links.append([url, (link, full_url)])  # Return parent page and document link as 2D array\n",
    "\n",
    "        # Now, find all internal links to recursively navigate\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "            if is_internal_link(full_url, base_url) and full_url not in visited:\n",
    "                doc_links.extend(get_document_links(full_url, base_url, visited))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching {url}: {e}\")\n",
    "\n",
    "    return doc_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST NO MAS BORRAR LUEGO\n",
    "\n",
    "def get_document_links(url, base_url, visited=None, link_limit=6):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    doc_links = []\n",
    "    link_count = 0  # Counter to track the number of document links found\n",
    "\n",
    "    # Ensure we do not revisit the same page more than once\n",
    "    if url in visited:\n",
    "        return doc_links\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.write(f\"Processing: {url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Skip binary files (like PDFs, DOCs) to avoid trying to parse them\n",
    "        if is_binary_content(response):\n",
    "            sys.stdout.write(f\"Skipping binary file: {url}\\r\")\n",
    "            sys.stdout.flush()\n",
    "            return doc_links\n",
    "\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all document links on the page (pdf, doc, docx)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith(('tel:', 'mailto:')):\n",
    "                continue\n",
    "            if href.endswith(('pdf', 'doc', 'docx', 'doc')):\n",
    "                full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "                doc_links.append([url, (link, full_url)])  # Return parent page and document link\n",
    "\n",
    "                link_count += 1\n",
    "                if link_count >= link_limit:\n",
    "                    print(\"Limit of 6 document links reached. Stopping.\")\n",
    "                    return doc_links\n",
    "\n",
    "        # Now, find all internal links to recursively navigate\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "            if is_internal_link(full_url, base_url) and full_url not in visited:\n",
    "                new_links = get_document_links(full_url, base_url, visited)\n",
    "                doc_links.extend(new_links)\n",
    "\n",
    "                link_count += len(new_links)\n",
    "                if link_count >= link_limit:\n",
    "                    print(\"Limit of 6 document links reached. Stopping.\")\n",
    "                    return doc_links\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching {url}: {e}\")\n",
    "\n",
    "    return doc_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir una cadena de texto a formato camelCase\n",
    "# THIS FUNCTION IS A COMPONENT OF scrape_and_store_links() function\n",
    "# camelCase, un estilo común de escritura en programación, especialmente útil para nombres de variables y funciones.\n",
    "# La función convierte texto a camelCase (ej. nombreCompleto), ideal para nombres de variables y claves en JSON, siguiendo convenciones de programación\n",
    "\n",
    "def to_camel_case(text):\n",
    "    # Return empty string if the input is empty or None\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'[_\\-]+', ' ', text)\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return ''\n",
    "    camel_case_text = words[0].lower() + ''.join(word.capitalize() for word in words[1:])\n",
    "    \n",
    "    return camel_case_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer y almacenar enlaces de documentos en JSON, incluyendo los segmentos de ruta de la página principal\n",
    "# Esta funcion crea el archivo .json \"scraped_links_y-m-d-H-M-S) \n",
    "# Extrae metadata parent_page, tittle, href y file_url\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp =datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    ##timestamp = datetime.now().isoformat()  # Timestamp for saving the file\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in doc_links:  # Extract parent_page and document link from the 2D array\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "            \n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to initiate scraping for documents\n",
    "# Función principal para iniciar la extracción de documentos desde un sitio web\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "\n",
    "\n",
    "def scrape_documents_from_website(url):\n",
    "    base_url = url  # The starting URL\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    doc_links = get_document_links(base_url, base_url, visited)\n",
    "\n",
    "    print(f\"Found {len(doc_links)} document links:\")\n",
    "    for title, link in doc_links:\n",
    "        print(f\"- {title}: {link}\")\n",
    "\n",
    "    return doc_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create subdirectories based on the parent page path segments\n",
    "# Función para crear subdirectorios basados en los segmentos de ruta de la página principal\n",
    "# This function organizes documents into a structured folder system based on the webpage hierarchy, making it easier to manage downloaded files according to their origin.\n",
    "# This function is called and a component for download_file() function\n",
    "\n",
    "\n",
    "def create_directory_structure(parent_page_segments, doc_url):\n",
    "    # Convert the path segments array into a directory path\n",
    "    directory_path = os.path.join(DOWNLOAD_DIR, *parent_page_segments)\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a document and save it in the appropriate subdirectory\n",
    "# Función para descargar un documento y guardarlo en el subdirectorio correspondiente\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "\n",
    "    # Create the directory structure based on Page and Section\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "\n",
    "    # Create the full file path to save the file\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"\\nDownloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to download {doc_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=\"scraping_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logging.info(\"Starting the scraping process...\")\n",
    "\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url, timeout=10)  # Add timeout here\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.warning(f\"Timeout occurred for {doc_url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {doc_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth with UTF-8-SIG encoding\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode(\"utf-8-sig\").decode(\"utf-8-sig\")  # Ensure UTF-8-SIG encoding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber with UTF-8-SIG encoding\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            # Encode and decode entire text as UTF-8-SIG after extraction\n",
    "            return text.encode(\"utf-8-sig\").decode(\"utf-8-sig\") if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client with UTF-8-SIG encoding\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        # Handle UTF-8-SIG encoding for accented characters\n",
    "        return doc_text.encode(\"utf-8-sig\", \"ignore\").decode(\"utf-8-sig\", \"ignore\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main domain (base URL) using the tld library\n",
    "# Extraer el dominio principal (URL base) usando la biblioteca tld\n",
    "# NO ESTAMOS USANDO ESTA FUNCION\n",
    "# NO ES ACTIVADA  \n",
    "\n",
    "def get_base_domain(url):\n",
    "    try:\n",
    "        # Extract the top-level domain (e.g., comunidadandina.org)\n",
    "        return get_fld(url, fix_protocol=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base domain from {url}: {e}\")\n",
    "        return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_documents_from_json(json_file):\n",
    "    # Define the replacements for common problematic characters, including newline\n",
    "    replacements = {\n",
    "        r'\\u00e1': 'á', r'\\u00e9': 'é', r'\\u00ed': 'í', r'\\u00f3': 'ó', r'\\u00fa': 'ú',\n",
    "        r'\\u00f1': 'ñ', r'\\u00c1': 'Á', r'\\u00c9': 'É', r'\\u00cd': 'Í', r'\\u00d3': 'Ó',\n",
    "        r'\\u00da': 'Ú', r'\\u00d1': 'Ñ', r'\\n': ' '  # Replace newline with a space\n",
    "    }\n",
    "    \n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        links_data = json.load(f)\n",
    "\n",
    "    for link_data in links_data:\n",
    "        doc_url = link_data['file_url']\n",
    "        href = link_data['href']\n",
    "        parent_page_segments = link_data['parent_page']  # Array of path segments\n",
    "\n",
    "        sys.stdout.write(f\"Downloading: {doc_url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Use the parent page path segments to create the folder structure\n",
    "        file_path = download_file(parent_page_segments, doc_url, href)  # Download the document using the parent page path segments\n",
    "\n",
    "        if file_path:\n",
    "            extracted_text = extract_text_from_file(file_path)  # Extract text from the file (if applicable)\n",
    "            # Update the JSON structure with the downloaded file path and content\n",
    "            link_data['downloaded_path'] = file_path\n",
    "            link_data['extracted_content'] = extracted_text or \"N/A\"\n",
    "\n",
    "    # Perform replacements using the dictionary\n",
    "    json_str = json.dumps(links_data)  # Convert JSON data to string\n",
    "    for unicode_seq, char in replacements.items():\n",
    "        json_str = re.sub(unicode_seq, char, json_str)\n",
    "    \n",
    "    # Convert the string back to JSON format\n",
    "    updated_json_data = json.loads(json_str)\n",
    "    \n",
    "    # Save updated data back to the JSON file with results\n",
    "    output_file = f\"results_with_download_{datetime.now().isoformat().replace(':', '-')}.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(updated_json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Downloads complete. Updated results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from urllib.parse import unquote, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_encoded_text(text):\n",
    "    # Use a regex to find and remove all (cid:xxx) patterns\n",
    "    return re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "\n",
    "def download_documents_from_json(json_file):\n",
    "    # Define the replacements for common problematic characters, including newline\n",
    "    replacements = {\n",
    "        r'\\u00e1': 'á', r'\\u00e9': 'é', r'\\u00ed': 'í', r'\\u00f3': 'ó', r'\\u00fa': 'ú',\n",
    "        r'\\u00f1': 'ñ', r'\\u00c1': 'Á', r'\\u00c9': 'É', r'\\u00cd': 'Í', r'\\u00d3': 'Ó',\n",
    "        r'\\u00da': 'Ú', r'\\u00d1': 'Ñ', r'\\n': ' '  # Replace newline with a space\n",
    "    }\n",
    "\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        links_data = json.load(f)\n",
    "\n",
    "    for link_data in links_data:\n",
    "        doc_url = link_data['file_url']\n",
    "        href = link_data['href']\n",
    "        parent_page_segments = link_data['parent_page']  # Array of path segments\n",
    "\n",
    "        sys.stdout.write(f\"Downloading: {doc_url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Use the parent page path segments to create the folder structure\n",
    "        file_path = download_file(parent_page_segments, doc_url, href)  # Download the document using the parent page path segments\n",
    "\n",
    "        if file_path:\n",
    "            extracted_text = extract_text_from_file(file_path)  # Extract text from the file (if applicable)\n",
    "            # Clean the extracted text from any (cid:xxx) patterns\n",
    "            cleaned_text = clean_encoded_text(extracted_text)\n",
    "            # Update the JSON structure with the downloaded file path and content\n",
    "            link_data['downloaded_path'] = file_path\n",
    "            link_data['extracted_content'] = cleaned_text or \"N/A\"\n",
    "\n",
    "    # Perform replacements using the dictionary\n",
    "    json_str = json.dumps(links_data)  # Convert JSON data to string\n",
    "    for unicode_seq, char in replacements.items():\n",
    "        json_str = re.sub(unicode_seq, char, json_str)\n",
    "    \n",
    "    # Convert the string back to JSON format\n",
    "    updated_json_data = json.loads(json_str)\n",
    "    \n",
    "    # Save updated data back to the JSON file with results\n",
    "    output_file = f\"results_with_download_{datetime.now().isoformat().replace(':', '-')}.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(updated_json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Downloads complete. Updated results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    for url in tqdm(urls, desc=\"Processing URLs\"):  # Add tqdm progress bar to URLs loop\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in tqdm(doc_links, desc=\"Extracting links\", leave=False):  # Inner progress bar for each URL\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "\n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BORRARRRR LUEGO \n",
    "\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    link_count = 0  # Counter to track the number of links scraped\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in doc_links:\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "\n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],\n",
    "                \"file_url\": doc_url,\n",
    "            })\n",
    "            \n",
    "            link_count += 1\n",
    "            # Stop after scraping 6 links\n",
    "            if link_count >= 6:\n",
    "                print(\"Limit of 6 links reached. Stopping.\")\n",
    "                break\n",
    "\n",
    "        if link_count >= 6:\n",
    "            break\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://produccion.gob.bo/\n",
      "Limit of 6 document links reached. Stopping.\n",
      "Limit of 6 links reached. Stopping.\n",
      "Scraping complete. Links saved to scraped_links_2024-10-18_00-30-06.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'scraped_links_2024-10-18_00-30-06.json'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrape_and_store_link\n",
    "\n",
    "urls = read_urls()\n",
    "scrape_and_store_links(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads complete. Updated results saved to results_with_download_2024-10-18T00-31-11.313225.jsondfdf\n"
     ]
    }
   ],
   "source": [
    "download_documents_from_json('scraped_links_2024-10-15_20-26-53.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
