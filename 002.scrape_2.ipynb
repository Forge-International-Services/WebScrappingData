{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mammoth\n",
    "import json\n",
    "from urllib.parse import urlparse, urljoin, unquote\n",
    "import re\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import docx\n",
    "import pypandoc\n",
    "\n",
    "from tld import get_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypandoc\n",
    "#pypandoc.download_pandoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for downloads\n",
    "DOWNLOAD_DIR = \"999_downloaded_documents_link1\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from file\n",
    "def read_urls(file_name='999.web_urls.txt'):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Función auxiliar para verificar si un enlace es interno (pertenece al mismo dominio)\n",
    "# Esta funcion es un componente del proceso en bloque 7 que \"scrape and store document links in JSON with parent page path segments\" ---> esta funcion evita enviar request a links por fuera del dominio al que estamos scrapping\n",
    "\n",
    "\n",
    "# Helper function to check if a link is internal (within the same domain)\n",
    "def is_internal_link(url, base_url):\n",
    "    # Parse both URLs\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "\n",
    "    # Check if the domain of the URL matches the base URL's domain\n",
    "    # TRUE si el dominio es el mismo que el base pero FULL URL, RELATIVE PATH, SAME DOMAIN BUT DIFFERENT PROTOCOL else FALSE\n",
    "    return parsed_url.netloc == parsed_base_url.netloc or parsed_url.netloc == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si el contenido de una respuesta es un formato binario (como PDF o Word)\n",
    "#'application/pdf' for PDFs,'application/msword' for Word documents, and 'application/vnd' for various document formats (e.g., Excel or OpenDocument).\n",
    "# THIS FUNCTION IS A COMPONENT FOR get_document_links() Function\n",
    "\n",
    "def is_binary_content(response):\n",
    "    content_type = response.headers.get('Content-Type', '').lower()\n",
    "    return 'application/pdf' in content_type or 'application/msword' in content_type or 'application/vnd' in content_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar nombres de archivos extraídos de URLs\n",
    "# The cleaned file_name is returned, making it suitable for use as a filename.\n",
    "# THIS FUNCTION IS A COMPONENT FOR download_file() Function\n",
    "\n",
    "# Function to clean filenames\n",
    "def clean_filename(href):\n",
    "    # Extract the file name from the URL\n",
    "    file_name = unquote(os.path.basename(urlparse(href).path))\n",
    "    # Remove invalid characters for filenames using a regex\n",
    "    file_name = re.sub(r'[<>:\"/\\\\|?*]', '_', file_name)\n",
    "    # Fallback if the URL doesn't have a file name\n",
    "    if not file_name:\n",
    "        file_name = \"downloaded_file\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener enlaces de documentos desde un sitio web, navegando recursivamente por enlaces internos.\n",
    "# THIS FUNCTION IS A COMPONEN FOR scrape_and_store_links() and scrape_documents_from_website() FUNCTIONS\n",
    "\n",
    "# Recursively fetch document links from a website, traversing internal links.\n",
    "def get_document_links(url, base_url, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    doc_links = []\n",
    "\n",
    "    # Ensure we do not revisit the same page more than once\n",
    "    if url in visited:\n",
    "        return doc_links\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.write(f\"Processing: {url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Skip binary files (like PDFs, DOCs) to avoid trying to parse them\n",
    "        if is_binary_content(response):\n",
    "            sys.stdout.write(f\"Skipping binary file: {url}\\r\")\n",
    "            sys.stdout.flush()\n",
    "            return doc_links\n",
    "\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all document links on the page (pdf, doc, docx)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith(('tel:', 'mailto:')):\n",
    "                continue\n",
    "            if href.endswith(('pdf', 'doc', 'docx','doc')):\n",
    "                full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "                # Append both the parent page (url) and the document link (full_url)\n",
    "                doc_links.append([url, (link, full_url)])  # Return parent page and document link as 2D array\n",
    "\n",
    "        # Now, find all internal links to recursively navigate\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "            if is_internal_link(full_url, base_url) and full_url not in visited:\n",
    "                doc_links.extend(get_document_links(full_url, base_url, visited))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching {url}: {e}\")\n",
    "\n",
    "    return doc_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose\n",
    "The function get_document_links() returns a list of document links found on the site, with each link paired with its originating page. This approach is useful for collecting downloadable files from an entire website, starting from a base URL and exploring all reachable internal pages.\n",
    "\n",
    "OUTCOME EXAMPLE: \n",
    "\n",
    "doc_links = \n",
    "[\n",
    "    [\n",
    "        'https://example.com/page1', \n",
    "        ('Document 1', 'https://example.com/files/doc1.pdf')\n",
    "    ],\n",
    "    [\n",
    "        'https://example.com/page1', \n",
    "        ('Document 2', 'https://example.com/files/doc2.doc')\n",
    "    ],\n",
    "    [\n",
    "        'https://example.com/page2', \n",
    "        ('Research Paper', 'https://example.com/files/research_paper.pdf')\n",
    "    ],\n",
    "    [\n",
    "        'https://example.com/page3', \n",
    "        ('Report 2023', 'https://example.com/files/report.docx')\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir una cadena de texto a formato camelCase\n",
    "# THIS FUNCTION IS A COMPONENT OF scrape_and_store_links() function\n",
    "# camelCase, un estilo común de escritura en programación, especialmente útil para nombres de variables y funciones.\n",
    "# La función convierte texto a camelCase (ej. nombreCompleto), ideal para nombres de variables y claves en JSON, siguiendo convenciones de programación\n",
    "\n",
    "def to_camel_case(text):\n",
    "    # Return empty string if the input is empty or None\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'[_\\-]+', ' ', text)\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return ''\n",
    "    camel_case_text = words[0].lower() + ''.join(word.capitalize() for word in words[1:])\n",
    "    \n",
    "    return camel_case_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer y almacenar enlaces de documentos en JSON, incluyendo los segmentos de ruta de la página principal\n",
    "# Esta funcion crea el archivo .json \"scraped_links_y-m-d-H-M-S) \n",
    "# Extrae metadata parent_page, tittle, href y file_url\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp =datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    ##timestamp = datetime.now().isoformat()  # Timestamp for saving the file\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in doc_links:  # Extract parent_page and document link from the 2D array\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "            \n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función scrape_and_store_links extrae la siguiente metadata para cada documento encontrado:\n",
    "\n",
    "timestamp: La fecha y hora exacta de la extracción, en formato YYYY-MM-DD_HH-MM-SS. Esto facilita saber cuándo se realizó el scraping.\n",
    "\n",
    "parent_page: Los segmentos de la ruta de la página principal donde se encontró el enlace. Cada segmento de la URL se guarda como un elemento en una lista, usando formato camelCase (ej. [\"seccionPrincipal\", \"subseccion\"]). Si no hay ruta, se usa el dominio.\n",
    "\n",
    "title: El texto del enlace al documento, que generalmente describe el documento o su título. Si está vacío, se establece como \"No title\".\n",
    "\n",
    "href: La URL parcial extraída directamente del atributo href del enlace (<a>) en la página.\n",
    "\n",
    "file_url: La URL completa del documento (PDF, DOC, etc.), que se puede utilizar para descargar el archivo.\n",
    "\n",
    "Este conjunto de metadata organiza los documentos y su procedencia, siendo útil para estructurar y consultar enlaces descargables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to initiate scraping for documents\n",
    "# Función principal para iniciar la extracción de documentos desde un sitio web\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "\n",
    "\n",
    "def scrape_documents_from_website(url):\n",
    "    base_url = url  # The starting URL\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    doc_links = get_document_links(base_url, base_url, visited)\n",
    "\n",
    "    print(f\"Found {len(doc_links)} document links:\")\n",
    "    for title, link in doc_links:\n",
    "        print(f\"- {title}: {link}\")\n",
    "\n",
    "    return doc_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGICA DE scrape_documents_from_website(x):\n",
    "\n",
    "Si llamamos a la función con un sitio web como https://example.com, el output en la consola podría verse así:\n",
    "scrape_documents_from_website(\"https://example.com\")\n",
    "\n",
    "OUTPUT IS A LIST OF TUPPLES\n",
    "Each tuple in the list contains two elements: the title of the document and the document’s URL.\n",
    "\n",
    "OUTPUT:\n",
    "Found 3 document links:\n",
    "- Document 1: https://example.com/files/doc1.pdf\n",
    "- Report 2022: https://example.com/files/report2022.doc\n",
    "- Guide: https://example.com/downloads/guide.docx\n",
    "\n",
    "Explicación\n",
    "La función muestra:\n",
    "\n",
    "El total de enlaces de documentos encontrados (en este caso, 3).\n",
    "Una lista donde cada línea representa un documento, mostrando su título (Document 1, Report 2022, Guide) y el enlace directo al archivo (doc1.pdf, report2022.doc, guide.docx).\n",
    "Este formato hace fácil ver qué documentos fueron encontrados y sus URLs exactas, útiles para confirmar que los enlaces fueron correctamente detectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create subdirectories based on the parent page path segments\n",
    "# Función para crear subdirectorios basados en los segmentos de ruta de la página principal\n",
    "# This function organizes documents into a structured folder system based on the webpage hierarchy, making it easier to manage downloaded files according to their origin.\n",
    "# This function is called and a component for download_file() function\n",
    "\n",
    "\n",
    "def create_directory_structure(parent_page_segments, doc_url):\n",
    "    # Convert the path segments array into a directory path\n",
    "    directory_path = os.path.join(DOWNLOAD_DIR, *parent_page_segments)\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a document and save it in the appropriate subdirectory\n",
    "# Función para descargar un documento y guardarlo en el subdirectorio correspondiente\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "\n",
    "    # Create the directory structure based on Page and Section\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "\n",
    "    # Create the full file path to save the file\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"\\nDownloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to download {doc_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting the scraping process...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=\"scraping_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logging.info(\"Starting the scraping process...\")\n",
    "\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url, timeout=10)  # Add timeout here\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.warning(f\"Timeout occurred for {doc_url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {doc_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from files (.docx, .doc, and .pdf supported)\n",
    "# Funciones para extraer texto de archivos (.docx, .doc y .pdf)\n",
    "# Componente que se llama cuando se aplica download_documents_from_json()\n",
    "\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            return result.value\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using PyPDF2 or pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        # Using pdfplumber for more robust text extraction from PDFs\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"  # Ensure we add non-None content\n",
    "            return text if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using pypandoc\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        # Using pypandoc to convert .doc to text\n",
    "        return pypandoc.convert_file(file_path, 'plain')\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            return result.value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"  # Ensure we add non-None content\n",
    "            return text if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return doc_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth with UTF-8 encoding\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode(\"utf-8\").decode(\"utf-8\")  # Ensure UTF-8 encoding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber with UTF-8 encoding\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text.encode(\"utf-8\").decode(\"utf-8\")  # UTF-8 encode/decode\n",
    "            return text if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client with UTF-8 encoding\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return doc_text.encode(\"utf-8\").decode(\"utf-8\")  # UTF-8 encode/decode\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main domain (base URL) using the tld library\n",
    "# Extraer el dominio principal (URL base) usando la biblioteca tld\n",
    "# NO ESTAMOS USANDO ESTA FUNCION\n",
    "# NO ES ACTIVADA  \n",
    "\n",
    "def get_base_domain(url):\n",
    "    try:\n",
    "        # Extract the top-level domain (e.g., comunidadandina.org)\n",
    "        return get_fld(url, fix_protocol=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base domain from {url}: {e}\")\n",
    "        return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the stored links and download documents into organized directories\n",
    "# Función para leer los enlaces almacenados, descargar documentos y organizar en directorios\n",
    "\n",
    "\n",
    "\n",
    "def download_documents_from_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        links_data = json.load(f)\n",
    "\n",
    "    for link_data in links_data:\n",
    "        doc_url = link_data['file_url']\n",
    "        href = link_data['href']\n",
    "        parent_page_segments = link_data['parent_page']  # Array of path segments\n",
    "\n",
    "        sys.stdout.write(f\"Downloading: {doc_url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Use the parent page path segments to create the folder structure\n",
    "        file_path = download_file(parent_page_segments, doc_url, href)  # Download the document using the parent page path segments\n",
    "\n",
    "        if file_path:\n",
    "            extracted_text = extract_text_from_file(file_path)  # Extract text from the file (if applicable)\n",
    "            # Update the JSON structure with the downloaded file path and content\n",
    "            link_data['downloaded_path'] = file_path\n",
    "            link_data['extracted_content'] = extracted_text or \"N/A\"\n",
    "\n",
    "    # Save updated data back to the JSON file with results\n",
    "    output_file = f\"results_with_download_{datetime.now().isoformat().replace(':', '-')}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(links_data, f, indent=4)\n",
    "\n",
    "    print(f\"Downloads complete. Updated results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, download_documents_from_json, reads a JSON file with document links, downloads each document, organizes them into folders, and extracts content if possible. Here’s how it works:\n",
    "\n",
    "Load JSON Data:\n",
    "\n",
    "with open(json_file, 'r') as f: links_data = json.load(f): Opens and reads the JSON file, loading the document link data into links_data.\n",
    "Process Each Document:\n",
    "\n",
    "For each document link in links_data, it retrieves:\n",
    "doc_url: The full URL of the document.\n",
    "href: The direct link to the document in the HTML.\n",
    "parent_page_segments: An array of path segments from the parent page URL, used to organize folders.\n",
    "Download and Extract Content:\n",
    "\n",
    "file_path = download_file(parent_page_segments, doc_url, href): Downloads the document, organizing it by folder based on the parent page’s path segments. If successful, the file path is returned.\n",
    "If the file is downloaded, extract_text_from_file(file_path) attempts to extract text from the document (if it’s a .pdf, .doc, or .docx).\n",
    "Update Metadata and Save:\n",
    "\n",
    "The downloaded path (downloaded_path) and extracted text (extracted_content) are added to link_data.\n",
    "Finally, the updated data is saved back into a new JSON file, results_with_download_<timestamp>.json, with each link entry containing paths and extracted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'get_document_links' and 'to_camel_case' functions are defined\n",
    "\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    for url in tqdm(urls, desc=\"Processing URLs\"):  # Add tqdm progress bar to URLs loop\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in tqdm(doc_links, desc=\"Extracting links\", leave=False):  # Inner progress bar for each URL\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "\n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth with UTF-8 handling for accented characters\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode(\"utf-8\").decode(\"utf-8\")  # Ensure UTF-8 encoding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber with UTF-8 encoding and explicit decode handling\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            # Encode and decode entire text as UTF-8 after extraction to handle accented characters\n",
    "            return text.encode(\"utf-8\").decode(\"utf-8\") if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client with UTF-8 encoding\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        # Handle UTF-8 encoding for accented characters\n",
    "        return doc_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")  # Ignore errors if encoding fails\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth with UTF-8-SIG encoding\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode(\"utf-8-sig\").decode(\"utf-8-sig\")  # Ensure UTF-8-SIG encoding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber with UTF-8-SIG encoding\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            # Encode and decode entire text as UTF-8-SIG after extraction\n",
    "            return text.encode(\"utf-8-sig\").decode(\"utf-8-sig\") if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client with UTF-8-SIG encoding\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        # Handle UTF-8-SIG encoding for accented characters\n",
    "        return doc_text.encode(\"utf-8-sig\", \"ignore\").decode(\"utf-8-sig\", \"ignore\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            return result.value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "\n",
    "            # Decode any Unicode escape sequences (e.g., \\u00f3) to readable text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client and clean Unicode escapes\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        \n",
    "        # Clean up Unicode escape sequences\n",
    "        return doc_text.encode(\"utf-8\", \"replace\").decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            # Decode any Unicode escape sequences (e.g., \\u00f3) to readable text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            # Decode any Unicode escape sequences (e.g., \\u00f3) to readable text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client and clean Unicode escapes\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        \n",
    "        # Clean up Unicode escape sequences\n",
    "        return doc_text.encode(\"utf-8\", \"replace\").decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return normalize_text(extract_text_from_docx(file_path))\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return normalize_text(extract_text_from_pdf(file_path))\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return normalize_text(extract_text_from_doc(file_path))\n",
    "    return None\n",
    "\n",
    "# Normalize extracted text to handle Unicode sequences\n",
    "def normalize_text(text):\n",
    "    if text:\n",
    "        # Normalize to NFC (Canonical Composition) to handle accented characters\n",
    "        return unicodedata.normalize('NFC', text)\n",
    "    return text\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            return result.value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return doc_text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Function to clean up any remaining unicode sequences\n",
    "def clean_unicode_sequences(text):\n",
    "    try:\n",
    "        return text.encode('utf-8').decode('utf-8-sig')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Unicode cleanup: {e}\")\n",
    "        return text\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return clean_unicode_sequences(text)  # Clean up Unicode sequences\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return clean_unicode_sequences(text) if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return clean_unicode_sequences(doc_text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Function to clean up any remaining unicode sequences\n",
    "def clean_unicode_sequences(text):\n",
    "    try:\n",
    "        return text.encode('utf-8').decode('utf-8-sig')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during Unicode cleanup: {e}\")\n",
    "        return text\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return clean_unicode_sequences(text)  # Clean up Unicode sequences\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return clean_unicode_sequences(text) if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return clean_unicode_sequences(doc_text)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return replace_unicode_escapes(extract_text_from_docx(file_path))\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return replace_unicode_escapes(extract_text_from_pdf(file_path))\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return replace_unicode_escapes(extract_text_from_doc(file_path))\n",
    "    return None\n",
    "\n",
    "# Replace specific Unicode escapes with actual characters\n",
    "def replace_unicode_escapes(text):\n",
    "    if text:\n",
    "        unicode_replacements = {\n",
    "            r'\\u00e1': 'á', r'\\u00e9': 'é', r'\\u00ed': 'í', r'\\u00f3': 'ó', r'\\u00fa': 'ú',\n",
    "            r'\\u00f1': 'ñ', r'\\u00c1': 'Á', r'\\u00c9': 'É', r'\\u00cd': 'Í', r'\\u00d3': 'Ó',\n",
    "            r'\\u00da': 'Ú', r'\\u00d1': 'Ñ'\n",
    "        }\n",
    "        for unicode_seq, char in unicode_replacements.items():\n",
    "            text = re.sub(unicode_seq, char, text)\n",
    "    return text\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            return result.value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files directly using win32com.client\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False  # Optional: Keep Word hidden\n",
    "        doc = word.Documents.Open(os.path.abspath(file_path))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return doc_text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the stored links and download documents into organized directories\n",
    "# Función para leer los enlaces almacenados, descargar documentos y organizar en directorios\n",
    "\n",
    "\n",
    "\n",
    "def download_documents_from_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        links_data = json.load(f)\n",
    "\n",
    "    for link_data in links_data:\n",
    "        doc_url = link_data['file_url']\n",
    "        href = link_data['href']\n",
    "        parent_page_segments = link_data['parent_page']  # Array of path segments\n",
    "\n",
    "        sys.stdout.write(f\"Downloading: {doc_url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Use the parent page path segments to create the folder structure\n",
    "        file_path = download_file(parent_page_segments, doc_url, href)  # Download the document using the parent page path segments\n",
    "\n",
    "        if file_path:\n",
    "            extracted_text = extract_text_from_file(file_path)  # Extract text from the file (if applicable)\n",
    "            # Update the JSON structure with the downloaded file path and content\n",
    "            link_data['downloaded_path'] = file_path\n",
    "            link_data['extracted_content'] = extracted_text or \"N/A\"\n",
    "\n",
    "    # Save updated data back to the JSON file with results\n",
    "    output_file = f\"results_with_download_{datetime.now().isoformat().replace(':', '-')}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(links_data, f, indent=4)\n",
    "\n",
    "    print(f\"Downloads complete. Updated results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_and_store_link\n",
    "\n",
    "urls = read_urls()\n",
    "scrape_and_store_links(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.comunidadandina.org/documents/quienes-somos/acuerdocartagena.pdf\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded: acuerdocartagena.pdf to 999_downloaded_documents_link1\\quienesSomos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.comunidadandina.org/documents/quienes-somos/secretaria-general/dec409.doc\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded: dec409.doc to 999_downloaded_documents_link1\\quienesSomos\\secretariaGeneralDeLaComunidadAndina\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.comunidadandina.org/StaticFiles/DocOf/DEC726.pdf\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded: DEC726.pdf to 999_downloaded_documents_link1\\temas\\dgDec\\cooperacionTecnica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.comunidadandina.org/DocOficialesFiles/Gacetas/Gace1787.pdf\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloaded: Gace1787.pdf to 999_downloaded_documents_link1\\temas\\dgDec\\cooperacionTecnica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads complete. Updated results saved to results_with_download_2024-10-17T18-19-41.369908.json\n"
     ]
    }
   ],
   "source": [
    "download_documents_from_json('scraped_links_2024-10-15_20-26-53.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character replacements in JSON file completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    # Define the replacements for common problematic characters\n",
    "    replacements = {\n",
    "        r'\\u00e1': 'á', r'\\u00e9': 'é', r'\\u00ed': 'í', r'\\u00f3': 'ó', r'\\u00fa': 'ú',\n",
    "        r'\\u00f1': 'ñ', r'\\u00c1': 'Á', r'\\u00c9': 'É', r'\\u00cd': 'Í', r'\\u00d3': 'Ó',\n",
    "        r'\\u00da': 'Ú', r'\\u00d1': 'Ñ'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Read the JSON file\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Step 2: Convert JSON to string and perform replacements\n",
    "        json_str = json.dumps(json_data)  # Convert JSON data to string\n",
    "        \n",
    "        # Perform replacements using the dictionary\n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "        \n",
    "        # Step 3: Convert the string back to JSON format\n",
    "        updated_json_data = json.loads(json_str)\n",
    "        \n",
    "        # Step 4: Write the updated JSON back to the file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Usage\n",
    "replace_characters_in_json_file('results_with_download_2024-10-17T18-19-41.369908.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función principal que organiza el flujo de scraping, almacenamiento y descarga de documentos\n",
    "def main():\n",
    "    urls = read_urls()  # Read URLs from the text file\n",
    "\n",
    "    # Phase 1: Scrape and store links in JSON\n",
    "    links_json_file = scrape_and_store_links(urls)\n",
    "    #links_json_file = \"scraped_links_2024-10-09.json\"\n",
    "\n",
    "    # Phase 2: Download documents using the stored links\n",
    "    download_documents_from_json(links_json_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
