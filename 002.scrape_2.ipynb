{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mammoth\n",
    "import json\n",
    "from urllib.parse import urlparse, urljoin, unquote\n",
    "import re\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import docx\n",
    "import pypandoc\n",
    "from tld import get_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for downloads\n",
    "DOWNLOAD_DIR = \"999_downloaded_documents\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from file\n",
    "def read_urls(file_name='999.web_urls.txt'):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check if a link is internal (within the same domain)\n",
    "def is_internal_link(url, base_url):\n",
    "    # Parse both URLs\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "\n",
    "    # Check if the domain of the URL matches the base URL's domain\n",
    "    return parsed_url.netloc == parsed_base_url.netloc or parsed_url.netloc == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_binary_content(response):\n",
    "    content_type = response.headers.get('Content-Type', '').lower()\n",
    "    return 'application/pdf' in content_type or 'application/msword' in content_type or 'application/vnd' in content_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean filenames\n",
    "def clean_filename(href):\n",
    "    # Extract the file name from the URL\n",
    "    file_name = unquote(os.path.basename(urlparse(href).path))\n",
    "    # Remove invalid characters for filenames using a regex\n",
    "    file_name = re.sub(r'[<>:\"/\\\\|?*]', '_', file_name)\n",
    "    # Fallback if the URL doesn't have a file name\n",
    "    if not file_name:\n",
    "        file_name = \"downloaded_file\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively fetch document links from a website, traversing internal links.\n",
    "def get_document_links(url, base_url, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    doc_links = []\n",
    "\n",
    "    # Ensure we do not revisit the same page\n",
    "    if url in visited:\n",
    "        return doc_links\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.write(f\"Processing: {url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Skip binary files (like PDFs, DOCs) to avoid trying to parse them\n",
    "        if is_binary_content(response):\n",
    "            sys.stdout.write(f\"Skipping binary file: {url}\\r\")\n",
    "            sys.stdout.flush()\n",
    "            return doc_links\n",
    "\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all document links on the page (pdf, doc, docx)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith(('tel:', 'mailto:')):\n",
    "                continue\n",
    "            if href.endswith(('pdf', 'doc', 'docx','doc')):\n",
    "                full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "                # Append both the parent page (url) and the document link (full_url)\n",
    "                doc_links.append([url, (link, full_url)])  # Return parent page and document link as 2D array\n",
    "\n",
    "        # Now, find all internal links to recursively navigate\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "            if is_internal_link(full_url, base_url) and full_url not in visited:\n",
    "                doc_links.extend(get_document_links(full_url, base_url, visited))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching {url}: {e}\")\n",
    "\n",
    "    return doc_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a string to camelCase\n",
    "def to_camel_case(text):\n",
    "    # Return empty string if the input is empty or None\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'[_\\-]+', ' ', text)\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return ''\n",
    "    camel_case_text = words[0].lower() + ''.join(word.capitalize() for word in words[1:])\n",
    "    \n",
    "    return camel_case_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape and store document links in JSON with parent page path segments\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp =datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    ##timestamp = datetime.now().isoformat()  # Timestamp for saving the file\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in doc_links:  # Extract parent_page and document link from the 2D array\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "            \n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to initiate scraping for documents\n",
    "def scrape_documents_from_website(url):\n",
    "    base_url = url  # The starting URL\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    doc_links = get_document_links(base_url, base_url, visited)\n",
    "\n",
    "    print(f\"Found {len(doc_links)} document links:\")\n",
    "    for title, link in doc_links:\n",
    "        print(f\"- {title}: {link}\")\n",
    "\n",
    "    return doc_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create subdirectories based on the parent page path segments\n",
    "def create_directory_structure(parent_page_segments, doc_url):\n",
    "    # Convert the path segments array into a directory path\n",
    "    directory_path = os.path.join(DOWNLOAD_DIR, *parent_page_segments)\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a document and save it in the appropriate subdirectory\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "\n",
    "    # Create the directory structure based on Page and Section\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "\n",
    "    # Create the full file path to save the file\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"\\nDownloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to download {doc_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from files (.docx, .doc, and .pdf supported)\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    return None\n",
    "\n",
    "# Extract text from .docx files using mammoth\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            return result.value\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using PyPDF2 or pdfplumber\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        # Using pdfplumber for more robust text extraction from PDFs\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"  # Ensure we add non-None content\n",
    "            return text if text else \"No text found in the PDF\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using pypandoc\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        # Using pypandoc to convert .doc to text\n",
    "        return pypandoc.convert_file(file_path, 'plain')\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main domain (base URL) using the tld library\n",
    "def get_base_domain(url):\n",
    "    try:\n",
    "        # Extract the top-level domain (e.g., comunidadandina.org)\n",
    "        return get_fld(url, fix_protocol=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base domain from {url}: {e}\")\n",
    "        return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the stored links and download documents into organized directories\n",
    "def download_documents_from_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        links_data = json.load(f)\n",
    "\n",
    "    for link_data in links_data:\n",
    "        doc_url = link_data['file_url']\n",
    "        href = link_data['href']\n",
    "        parent_page_segments = link_data['parent_page']  # Array of path segments\n",
    "\n",
    "        sys.stdout.write(f\"Downloading: {doc_url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Use the parent page path segments to create the folder structure\n",
    "        file_path = download_file(parent_page_segments, doc_url, href)  # Download the document using the parent page path segments\n",
    "\n",
    "        if file_path:\n",
    "            extracted_text = extract_text_from_file(file_path)  # Extract text from the file (if applicable)\n",
    "            # Update the JSON structure with the downloaded file path and content\n",
    "            link_data['downloaded_path'] = file_path\n",
    "            link_data['extracted_content'] = extracted_text or \"N/A\"\n",
    "\n",
    "    # Save updated data back to the JSON file with results\n",
    "    output_file = f\"results_with_download_{datetime.now().isoformat().replace(':', '-')}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(links_data, f, indent=4)\n",
    "\n",
    "    print(f\"Downloads complete. Updated results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://www.comunidadandina.org/\n",
      "Processing: tel:+5117106573unidadandina.org/archivo-de-convocatorias/e-la-secretaria-general-can//a/\n",
      "Error fetching tel:+5117106573: No connection adapters were found for 'tel:+5117106573'\n",
      "Processing: mailto:bibliocan@comunidadandina.org\n",
      "Error fetching mailto:bibliocan@comunidadandina.org: No connection adapters were found for 'mailto:bibliocan@comunidadandina.org'\n",
      "Processing: tel:5117106400munidadandina.org/images/quienes-somos/simbolos-de-la-can/files/MAPA_CAN_PDF_155x230.zipA-CERTIFICACION-Y-VERIFICACION-DE-ORIGEN-2024.xlsxfdf\n",
      "Error fetching tel:5117106400: No connection adapters were found for 'tel:5117106400'\n",
      "Processing: mailto:contacto@conhu.org.perg/quienes-somos/organismo-andino-de-salud-convenio-hipolito-unanue/s/dfA-PRO-TEMPORE-DEL-ECUADOR.-Publicaci√≥n.pdfcamino-para-enfrentar-la-pandemia/\n",
      "Error fetching mailto:contacto@conhu.org.pe: No connection adapters were found for 'mailto:contacto@conhu.org.pe'\n",
      "Processing: mailto:infocaf@caf.comdina.org/quienes-somos/banco-de-desarrollo-de-america-latina/\n",
      "Error fetching mailto:infocaf@caf.com: No connection adapters were found for 'mailto:infocaf@caf.com'\n",
      "Processing: mailto:flar@flar.netandina.org/quienes-somos/fondo-latinoamericano-de-reservas/\n",
      "Error fetching mailto:flar@flar.net: No connection adapters were found for 'mailto:flar@flar.net'\n",
      "Processing: mailto:uasb@uasb.edu.boina.org/quienes-somos/universidad-andina-simon-bolivar/\n",
      "Error fetching mailto:uasb@uasb.edu.bo: No connection adapters were found for 'mailto:uasb@uasb.edu.bo'\n",
      "Processing: mailto:uasb@uasb.edu.ec\n",
      "Error fetching mailto:uasb@uasb.edu.ec: No connection adapters were found for 'mailto:uasb@uasb.edu.ec'\n",
      "Processing: mailto:tjca@tribunalandino.org.ecienes-somos/tribunal-de-justicia-de-la-comunidad-andina/acion-sai/Acreditados-CCEA-periodo-2024-2025-Relacion-de-miembros-acreditados.xlsx\n",
      "Error fetching mailto:tjca@tribunalandino.org.ec: No connection adapters were found for 'mailto:tjca@tribunalandino.org.ec'\n",
      "Processing: mailto:carias@parlamentoandino.orgenes-somos/parlamento-andino/\n",
      "Error fetching mailto:carias@parlamentoandino.org: No connection adapters were found for 'mailto:carias@parlamentoandino.org'\n",
      "Scraping complete. Links saved to scraped_links_2024-10-10_11-18-50.jsonocial-y-ciudadania-andina/ina/acion-sai/Acreditados-CCEA-periodo-2024-2025-Relacion-de-miembros-acreditados.xlsx\n",
      "Downloading: https://www.comunidadandina.org/documents/quienes-somos/acuerdocartagena.pdf\n",
      "Downloaded: acuerdocartagena.pdf to 999_downloaded_documents\\quienesSomos\n",
      "Downloading: https://www.comunidadandina.org/documents/quienes-somos/secretaria-general/dec409.doc\n",
      "Downloaded: dec409.doc to 999_downloaded_documents\\quienesSomos\\secretariaGeneralDeLaComunidadAndina\n",
      "Error extracting text from .doc file 999_downloaded_documents\\quienesSomos\\secretariaGeneralDeLaComunidadAndina\\dec409.doc: No pandoc was found: either install pandoc and add it\n",
      "to your PATH or or call pypandoc.download_pandoc(...) or\n",
      "install pypandoc wheels with included pandoc.\n",
      "Downloading: https://www.comunidadandina.org/StaticFiles/DocOf/DEC726.pdf\n",
      "Downloaded: DEC726.pdf to 999_downloaded_documents\\temas\\dgDec\\cooperacionTecnica\n",
      "Downloading: https://www.comunidadandina.org/DocOficialesFiles/Gacetas/Gace1787.pdf\n",
      "Downloaded: Gace1787.pdf to 999_downloaded_documents\\temas\\dgDec\\cooperacionTecnica\n",
      "Downloading: https://www.comunidadandina.org/StaticFiles/DocOf/DEC759.pdf\n",
      "Downloaded: DEC759.pdf to 999_downloaded_documents\\temas\\dgDec\\cooperacionTecnica\n",
      "Downloading: https://www.comunidadandina.org/StaticFiles/DocOf/DEC792.pdf\n",
      "Downloaded: DEC792.pdf to 999_downloaded_documents\\temas\\dgDec\\cooperacionTecnica\n",
      "Downloading: https://www.comunidadandina.org/DocOficialesFiles/Gacetas/GACE2400.pdf\n",
      "Downloaded: GACE2400.pdf to 999_downloaded_documents\\temas\\dgDec\\cooperacionTecnica\n",
      "Downloading: https://www.comunidadandina.org/wp-content/uploads/2022/01/CadenasdeValorRegionalCAN.pdf\r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    urls = read_urls()  # Read URLs from the text file\n",
    "\n",
    "    # Phase 1: Scrape and store links in JSON\n",
    "    links_json_file = scrape_and_store_links(urls)\n",
    "    #links_json_file = \"scraped_links_2024-10-09.json\"\n",
    "\n",
    "    # Phase 2: Download documents using the stored links\n",
    "    download_documents_from_json(links_json_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
