{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: DECISION 001\n",
      "Description: DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA\n",
      "Date: 25/11/1969\n",
      "Pages: 1\n",
      "Desígnese con el nombre de \"Acuerdo de Cartagena\" al Acuerdo de Integración Subregional suscrito por los Gobiernos de Bolivia, Colombia, Chile, Ecuador y Perú, en Bogotá, el 26 de mayo de 1969.\n",
      "\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34792\\99454274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;31m# Extract content from the .doc or .docx file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_and_parse_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Extract metadata from the HTML snippet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34792\\99454274.py\u001b[0m in \u001b[0;36mdownload_and_parse_doc\u001b[1;34m(url, filename)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mfile_extension\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".docx\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdocx_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmammoth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_raw_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocx_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mdoc_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\mammoth\\__init__.py\u001b[0m in \u001b[0;36mextract_raw_text\u001b[1;34m(fileobj)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_raw_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdocx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_raw_text_from_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\mammoth\\docx\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(fileobj)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mzip_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mpart_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_find_part_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     read_part_with_body = _part_with_body_reader(\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\mammoth\\zips.py\u001b[0m in \u001b[0;36mopen_zip\u001b[1;34m(fileobj, mode)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_Zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1333\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import mammoth\n",
    "import win32com.client as win32\n",
    "\n",
    "# URLs of the .doc/.docx files and corresponding metadata\n",
    "doc_data = [\n",
    "    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 001</div>\n",
    "                <div>DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA </div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.docx\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 002</div>\n",
    "                <div>DELEGACIÓN DE FACULTADES DE LA COMISIÓN A LA JUNTA</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to download and extract content from .doc or .docx files\n",
    "def download_and_parse_doc(url, filename):\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    # Determine file extension\n",
    "    file_extension = os.path.splitext(filename)[1].lower()\n",
    "    \n",
    "    # Handle .doc files using win32com\n",
    "    if file_extension == \".doc\":\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(os.path.abspath(filename))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "    # Handle .docx files using mammoth\n",
    "    elif file_extension == \".docx\":\n",
    "        with open(filename, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            doc_text = result.value\n",
    "    \n",
    "    return doc_text\n",
    "\n",
    "# Function to extract metadata including description from HTML snippet\n",
    "def extract_metadata(html_snippet):\n",
    "    soup = BeautifulSoup(html_snippet, \"html.parser\")\n",
    "    \n",
    "    title = soup.find(\"div\", class_=\"title\").text.strip()\n",
    "    description = soup.find_all(\"div\")[1].text.strip()  # The second <div> contains the brief description\n",
    "    date = soup.find_all(\"td\")[-1].text.strip()  # Last <td> contains the date\n",
    "    pages = soup.find(\"div\", class_=\"metadata\").find_all(\"strong\")[1].text.strip()  # Second <strong> contains the page number\n",
    "    \n",
    "    return title, description, date, pages\n",
    "\n",
    "# Process each document and its metadata\n",
    "for i, doc in enumerate(doc_data):\n",
    "    # Define the filename for saving the .doc/.docx file temporarily\n",
    "    filename = f\"temp_doc_{i+1}{os.path.splitext(doc['url'])[1]}\"\n",
    "    \n",
    "    # Extract content from the .doc or .docx file\n",
    "    content = download_and_parse_doc(doc[\"url\"], filename)\n",
    "    \n",
    "    # Extract metadata from the HTML snippet\n",
    "    title, description, date, pages = extract_metadata(doc[\"metadata\"])\n",
    "    \n",
    "    # Print or process the combined results\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Pages: {pages}\")\n",
    "    print(f\"Document Content: {content}\\n\")\n",
    "    \n",
    "    # Remove the temporary file\n",
    "    os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: DECISION 001\n",
      "Description: DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA\n",
      "Date: 25/11/1969\n",
      "Pages: 1\n",
      "Desígnese con el nombre de \"Acuerdo de Cartagena\" al Acuerdo de Integración Subregional suscrito por los Gobiernos de Bolivia, Colombia, Chile, Ecuador y Perú, en Bogotá, el 26 de mayo de 1969.\n",
      "\n",
      "File temp_doc_2.docx is not a valid .docx file.\n",
      "Title: DECISION 002\n",
      "Description: DELEGACIÓN DE FACULTADES DE LA COMISIÓN A LA JUNTA\n",
      "Date: 25/11/1969\n",
      "Pages: 1\n",
      "Document Content: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import mammoth\n",
    "import win32com.client as win32\n",
    "from zipfile import BadZipFile\n",
    "import mimetypes\n",
    "\n",
    "# URLs of the .doc/.docx files and corresponding metadata\n",
    "doc_data = [\n",
    "    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 001</div>\n",
    "                <div>DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA </div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.docx\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 002</div>\n",
    "                <div>DELEGACIÓN DE FACULTADES DE LA COMISIÓN A LA JUNTA</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to determine if a file is a valid .docx\n",
    "def is_valid_docx(filename):\n",
    "    try:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            if file.read(2) == b'PK':  # Check for ZIP file signature\n",
    "                return True\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking .docx file validity: {e}\")\n",
    "        return False\n",
    "\n",
    "# Function to download and extract content from .doc or .docx files\n",
    "def download_and_parse_doc(url, filename):\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    # Determine file extension\n",
    "    file_extension = os.path.splitext(filename)[1].lower()\n",
    "    \n",
    "    # Handle .doc files using win32com\n",
    "    if file_extension == \".doc\":\n",
    "        try:\n",
    "            word = win32.Dispatch(\"Word.Application\")\n",
    "            word.Visible = False\n",
    "            doc = word.Documents.Open(os.path.abspath(filename))\n",
    "            doc_text = doc.Content.Text\n",
    "            doc.Close()\n",
    "            word.Quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing .doc file: {e}\")\n",
    "            doc_text = \"\"\n",
    "    # Handle .docx files using mammoth\n",
    "    elif file_extension == \".docx\":\n",
    "        if is_valid_docx(filename):\n",
    "            try:\n",
    "                with open(filename, \"rb\") as docx_file:\n",
    "                    result = mammoth.extract_raw_text(docx_file)\n",
    "                    doc_text = result.value\n",
    "            except BadZipFile as e:\n",
    "                print(f\"Error processing .docx file (bad zip): {e}\")\n",
    "                doc_text = \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing .docx file: {e}\")\n",
    "                doc_text = \"\"\n",
    "        else:\n",
    "            print(f\"File {filename} is not a valid .docx file.\")\n",
    "            doc_text = \"\"\n",
    "    \n",
    "    return doc_text\n",
    "\n",
    "# Function to extract metadata including description from HTML snippet\n",
    "def extract_metadata(html_snippet):\n",
    "    soup = BeautifulSoup(html_snippet, \"html.parser\")\n",
    "    \n",
    "    title = soup.find(\"div\", class_=\"title\").text.strip()\n",
    "    description = soup.find_all(\"div\")[1].text.strip()  # The second <div> contains the brief description\n",
    "    date = soup.find_all(\"td\")[-1].text.strip()  # Last <td> contains the date\n",
    "    pages = soup.find(\"div\", class_=\"metadata\").find_all(\"strong\")[1].text.strip()  # Second <strong> contains the page number\n",
    "    \n",
    "    return title, description, date, pages\n",
    "\n",
    "# Process each document and its metadata\n",
    "for i, doc in enumerate(doc_data):\n",
    "    # Define the filename for saving the .doc/.docx file temporarily\n",
    "    filename = f\"temp_doc_{i+1}{os.path.splitext(doc['url'])[1]}\"\n",
    "    \n",
    "    # Extract content from the .doc or .docx file\n",
    "    content = download_and_parse_doc(doc[\"url\"], filename)\n",
    "    \n",
    "    # Extract metadata from the HTML snippet\n",
    "    title, description, date, pages = extract_metadata(doc[\"metadata\"])\n",
    "    \n",
    "    # Print or process the combined results\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Pages: {pages}\")\n",
    "    print(f\"Document Content: {content}\\n\")\n",
    "    \n",
    "    # Remove the temporary file\n",
    "    #s.remove(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: DECISION 001\n",
      "Description: DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA\n",
      "Date: 25/11/1969\n",
      "Pages: 1\n",
      "Document Content: \n",
      "\n",
      "Title: DECISION 002\n",
      "Description: DELEGACIÓN DE FACULTADES DE LA COMISIÓN A LA JUNTA\n",
      "Date: 25/11/1969\n",
      "Pages: 1\n",
      "Document Content: \n",
      "\n",
      "Title: DECISION 003\n",
      "Description: APROBACIÓN DEL PLAN DE ACCIÓN PARA EL AÑO 1970\n",
      "Date: 30/12/1969\n",
      "Pages: 2\n",
      "Document Content: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import mammoth\n",
    "import docx\n",
    "import win32com.client as win32\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "# URLs of the .doc/.docx files and corresponding metadata\n",
    "doc_data = [\n",
    "    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 001</div>\n",
    "                <div>DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA </div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 002</div>\n",
    "                <div>DELEGACIÓN DE FACULTADES DE LA COMISIÓN A LA JUNTA</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    },    {\n",
    "        \"url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC003.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 003</div>\n",
    "                <div>APROBACIÓN DEL PLAN DE ACCIÓN PARA EL AÑO 1970</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>25 Kb</strong>, Nro.Páginas: <strong>2</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC003.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>30/12/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to download and extract content from .doc or .docx files\n",
    "def download_and_parse_doc(url, filename):\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    # Try to process as .docx first\n",
    "    try:\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "        doc_text = \"\"\n",
    "        \n",
    "        if file_extension == \".docx\":\n",
    "            try:\n",
    "                # Try using python-docx\n",
    "                doc = docx.Document(filename)\n",
    "                doc_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            except Exception as e:\n",
    "                print(f\"Error using python-docx for .docx file: {e}\")\n",
    "                \n",
    "                # Fallback to mammoth if python-docx fails\n",
    "                try:\n",
    "                    with open(filename, \"rb\") as docx_file:\n",
    "                        result = mammoth.extract_raw_text(docx_file)\n",
    "                        doc_text = result.value\n",
    "                except BadZipFile as e:\n",
    "                    print(f\"Error processing .docx file (bad zip): {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing .docx file with mammoth: {e}\")\n",
    "                    \n",
    "        return doc_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .docx file: {e}\")\n",
    "    \n",
    "    # If .docx processing fails, retry with .doc\n",
    "    try:\n",
    "        file_extension = os.path.splitext(filename)[1].lower()\n",
    "        if file_extension == \".docx\":\n",
    "            word = win32.Dispatch(\"Word.Application\")\n",
    "            word.Visible = False\n",
    "            doc = word.Documents.Open(os.path.abspath(filename))\n",
    "            doc_text = doc.Content.Text\n",
    "            doc.Close()\n",
    "            word.Quit()\n",
    "        return doc_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract metadata including description from HTML snippet\n",
    "def extract_metadata(html_snippet):\n",
    "    soup = BeautifulSoup(html_snippet, \"html.parser\")\n",
    "    \n",
    "    title = soup.find(\"div\", class_=\"title\").text.strip()\n",
    "    description = soup.find_all(\"div\")[1].text.strip()  # The second <div> contains the brief description\n",
    "    date = soup.find_all(\"td\")[-1].text.strip()  # Last <td> contains the date\n",
    "    pages = soup.find(\"div\", class_=\"metadata\").find_all(\"strong\")[1].text.strip()  # Second <strong> contains the page number\n",
    "    \n",
    "    return title, description, date, pages\n",
    "\n",
    "# Process each document and its metadata\n",
    "for i, doc in enumerate(doc_data):\n",
    "    # Define the filename for saving the .doc/.docx file temporarily\n",
    "    filename = f\"temp_doc_{i+1}{os.path.splitext(doc['url'])[1]}\"\n",
    "    \n",
    "    # Extract content from the .doc or .docx file\n",
    "    content = download_and_parse_doc(doc[\"url\"], filename)\n",
    "    \n",
    "    # Extract metadata from the HTML snippet\n",
    "    title, description, date, pages = extract_metadata(doc[\"metadata\"])\n",
    "    \n",
    "    # Print or process the combined results\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Pages: {pages}\")\n",
    "    print(f\"Document Content: {content}\\n\")\n",
    "    \n",
    "    # Remove the temporary file\n",
    "    #os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to document_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import win32com.client as win32\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# URLs of the .doc files and corresponding metadata\n",
    "doc_data = [\n",
    "    {\n",
    "        \"source_url\": \"https://www.comunidadandina.org/normativa-andina/documentos-estadisticos/\",\n",
    "        \"doc_url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISIÓN 001</div>\n",
    "                <div>DESIGNACIÓN DEL ACUERDO DE INTEGRACIÓN SUBREGIONAL CON EL NOMBRE DE ACUERDO DE CARTAGENA</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC001.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    },\n",
    "    {\n",
    "        \"source_url\": \"https://www.comunidadandina.org/normativa-andina/documentos-estadisticos/\",\n",
    "        \"doc_url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISIÓN 002</div>\n",
    "                <div>DELEGACIÓN DE FACULTADES DE LA COMISIÓN A LA JUNTA</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>20 Kb</strong>, Nro.Páginas: <strong>1</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC002.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>25/11/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    },\n",
    "        {\n",
    "        \"source_url\": \"https://www.comunidadandina.org/normativa-andina/documentos-estadisticos/\",\n",
    "        \"doc_url\": \"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC003.doc\",\n",
    "        \"metadata\": '''\n",
    "        <tr>\n",
    "            <td colspan=\"2\">\n",
    "                <div class=\"title\">DECISION 003</div>\n",
    "                <div>APROBACIÓN DEL PLAN DE ACCIÓN PARA EL AÑO 1970</div>\n",
    "                <div class=\"metadata\">Tamaño: <strong>25 Kb</strong>, Nro.Páginas: <strong>2</strong></div>\n",
    "            </td>\n",
    "            <td class=\"text-center\">\n",
    "                <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.comunidadandina.org/DocOficialesFiles/decisiones/DEC003.doc\">\n",
    "                    <i class=\"bx bx bxs-file-doc fs-2\"></i>\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>30/12/1969</td>\n",
    "        </tr>\n",
    "        '''\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to download and extract content from .doc files\n",
    "def download_and_parse_doc(url, filename):\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    # Process as .doc file\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(os.path.abspath(filename))\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return doc_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing .doc file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract metadata including description from HTML snippet\n",
    "def extract_metadata(html_snippet):\n",
    "    soup = BeautifulSoup(html_snippet, \"html.parser\")\n",
    "    title = soup.find(\"div\", class_=\"title\").text.strip()\n",
    "    description = soup.find_all(\"div\")[1].text.strip()  # Second <div> for description\n",
    "    date = soup.find_all(\"td\")[-1].text.strip()  # Last <td> for date\n",
    "    pages = soup.find(\"div\", class_=\"metadata\").find_all(\"strong\")[1].text.strip()  # Second <strong> for page number\n",
    "    return title, description, date, pages\n",
    "\n",
    "# Create an empty list to store document data\n",
    "data = []\n",
    "\n",
    "# Get today's date for date_scraped\n",
    "today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Process each document and its metadata\n",
    "for i, doc in enumerate(doc_data):\n",
    "    # Define the filename for saving the .doc file temporarily\n",
    "    filename = f\"temp_doc_{i+1}.doc\"\n",
    "    \n",
    "    # Extract content from the .doc file\n",
    "    content = download_and_parse_doc(doc[\"doc_url\"], filename)\n",
    "    \n",
    "    # Extract metadata from the HTML snippet\n",
    "    title, description, date, pages = extract_metadata(doc[\"metadata\"])\n",
    "    \n",
    "    # Append the results to the list\n",
    "    data.append({\n",
    "        \"ID\": i + 1,\n",
    "        \"Source_URL\": doc[\"source_url\"],\n",
    "        \"Document_URL\": doc[\"doc_url\"],\n",
    "        \"Fecha_de_Extracción\": today_date,\n",
    "        \"Título\": title,\n",
    "        \"Descripción\": description,\n",
    "        \"Fecha_de_Publicación\": date,\n",
    "        \"Páginas\": pages,\n",
    "        \"Contenido_del_Documento\": content\n",
    "    })\n",
    "    \n",
    "    # Remove the temporary file\n",
    "    os.remove(filename)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(\"comunidad_andina_test.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Data has been saved to document_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table found.\n",
      "Cells: ['Sin información']\n",
      "Skipping row due to insufficient columns.\n",
      "Empty DataFrame\n",
      "Columns: [Title, Description, File Link, Publication Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://www.comunidadandina.org/normativa-andina/decisiones/'\n",
    "\n",
    "# Send a request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the document table\n",
    "table = soup.find('table')\n",
    "\n",
    "# Check if table is found\n",
    "if not table:\n",
    "    print(\"Table not found.\")\n",
    "else:\n",
    "    print(\"Table found.\")\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "titles = []\n",
    "descriptions = []\n",
    "file_links = []\n",
    "dates = []\n",
    "\n",
    "# Process each row in the table\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Debug: Print the cells\n",
    "    print(f\"Cells: {[cell.get_text(strip=True) for cell in cells]}\")\n",
    "    \n",
    "    if len(cells) < 3:\n",
    "        print(\"Skipping row due to insufficient columns.\")\n",
    "        continue  # Skip rows that don't have enough columns\n",
    "    \n",
    "    # Extract the title and description\n",
    "    title_div = cells[0].find('div', class_='title')\n",
    "    description_divs = cells[0].find_all('div')\n",
    "    \n",
    "    title = title_div.get_text(strip=True) if title_div else 'N/A'\n",
    "    description = description_divs[1].get_text(strip=True) if len(description_divs) > 1 else 'N/A'\n",
    "    \n",
    "    # Extract metadata\n",
    "    metadata_div = cells[0].find('div', class_='metadata')\n",
    "    size_match = re.search(r'Tamaño:\\s*<strong>(.*?)</strong>', str(metadata_div)) if metadata_div else None\n",
    "    pages_match = re.search(r'Nro.Páginas:\\s*<strong>(.*?)</strong>', str(metadata_div)) if metadata_div else None\n",
    "    size = size_match.group(1) if size_match else 'N/A'\n",
    "    pages = pages_match.group(1) if pages_match else 'N/A'\n",
    "    \n",
    "    # Extract the file link and date\n",
    "    file_link_tag = cells[1].find('a')\n",
    "    file_link = urljoin(url, file_link_tag['href']) if file_link_tag else 'N/A'\n",
    "    date = cells[2].get_text(strip=True) if len(cells) > 2 else 'N/A'\n",
    "    \n",
    "    # Debug: Print extracted information\n",
    "    print(f\"Title: {title}, Description: {description}, File Link: {file_link}, Date: {date}\")\n",
    "    \n",
    "    titles.append(title)\n",
    "    descriptions.append(description)\n",
    "    file_links.append(file_link)\n",
    "    dates.append(date)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Description': descriptions,\n",
    "    'File Link': file_links,\n",
    "    'Publication Date': dates\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('comunidad_andina_documents.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells: []\n",
      "Cells: ['Sin información']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the table rows\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Debug: Print the cells\n",
    "    print(f\"Cells: {[cell.get_text(strip=True) for cell in cells]}\")\n",
    "    \n",
    "    if len(cells) < 3:\n",
    "        continue  # Skip rows that don't have enough columns\n",
    "\n",
    "    # Adjust extraction logic based on actual content\n",
    "    # Extract title, description, and other info based on the structure you find\n",
    "    title = cells[0].get_text(strip=True)\n",
    "    description = cells[1].get_text(strip=True) if len(cells) > 1 else 'N/A'\n",
    "    file_link = cells[2].find('a')['href'] if len(cells) > 2 and cells[2].find('a') else 'N/A'\n",
    "    date = cells[3].get_text(strip=True) if len(cells) > 3 else 'N/A'\n",
    "\n",
    "    # Print extracted information\n",
    "    print(f\"Title: {title}, Description: {description}, File Link: {file_link}, Date: {date}\")\n",
    "\n",
    "    titles.append(title)\n",
    "    descriptions.append(description)\n",
    "    file_links.append(file_link)\n",
    "    dates.append(date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34792\\3176818427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mService\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mChromeService\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwebdriver_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchrome\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Open the webpage\n",
    "driver.get('https://www.comunidadandina.org/normativa-andina/decisiones/')\n",
    "\n",
    "# Extract table content\n",
    "table = driver.find_element(By.TAG_NAME, 'table')\n",
    "rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "titles = []\n",
    "descriptions = []\n",
    "file_links = []\n",
    "dates = []\n",
    "\n",
    "# Process each row in the table\n",
    "for row in rows[1:]:  # Skip the header row\n",
    "    cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "    \n",
    "    if len(cells) < 3:\n",
    "        continue  # Skip rows that don't have enough columns\n",
    "    \n",
    "    # Extract and print data\n",
    "    title = cells[0].text\n",
    "    description = cells[1].text if len(cells) > 1 else 'N/A'\n",
    "    file_link = cells[2].find_element(By.TAG_NAME, 'a').get_attribute('href') if len(cells) > 2 and cells[2].find_element(By.TAG_NAME, 'a') else 'N/A'\n",
    "    date = cells[3].text if len(cells) > 3 else 'N/A'\n",
    "    \n",
    "    print(f\"Title: {title}, Description: {description}, File Link: {file_link}, Date: {date}\")\n",
    "\n",
    "    titles.append(title)\n",
    "    descriptions.append(description)\n",
    "    file_links.append(file_link)\n",
    "    dates.append(date)\n",
    "\n",
    "# Create and save DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Description': descriptions,\n",
    "    'File Link': file_links,\n",
    "    'Publication Date': dates\n",
    "})\n",
    "df.to_csv('comunidad_andina_documents.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4174414626.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_34792\\4174414626.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install selenium\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
