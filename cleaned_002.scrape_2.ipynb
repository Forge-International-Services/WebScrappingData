{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e5a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mammoth\n",
    "import json\n",
    "from urllib.parse import urlparse, urljoin, unquote\n",
    "import re\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import docx\n",
    "import pypandoc\n",
    "\n",
    "from tld import get_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198890e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypandoc\n",
    "#pypandoc.download_pandoc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12842b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for downloads\n",
    "DOWNLOAD_DIR = \"999_downloaded_documents_link1\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc39085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read URLs from file\n",
    "def read_urls(file_name='999.web_urls.txt'):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e4ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---> Función auxiliar para verificar si un enlace es interno (pertenece al mismo dominio)\n",
    "# Esta funcion es un componente del proceso en bloque 7 que \"scrape and store document links in JSON with parent page path segments\" ---> esta funcion evita enviar request a links por fuera del dominio al que estamos scrapping\n",
    "\n",
    "\n",
    "# Helper function to check if a link is internal (within the same domain)\n",
    "def is_internal_link(url, base_url):\n",
    "    # Parse both URLs\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "\n",
    "    # Check if the domain of the URL matches the base URL's domain\n",
    "    # TRUE si el dominio es el mismo que el base pero FULL URL, RELATIVE PATH, SAME DOMAIN BUT DIFFERENT PROTOCOL else FALSE\n",
    "    return parsed_url.netloc == parsed_base_url.netloc or parsed_url.netloc == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e902e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica si el contenido de una respuesta es un formato binario (como PDF o Word)\n",
    "#'application/pdf' for PDFs,'application/msword' for Word documents, and 'application/vnd' for various document formats (e.g., Excel or OpenDocument).\n",
    "# THIS FUNCTION IS A COMPONENT FOR get_document_links() Function\n",
    "\n",
    "def is_binary_content(response):\n",
    "    content_type = response.headers.get('Content-Type', '').lower()\n",
    "    return 'application/pdf' in content_type or 'application/msword' in content_type or 'application/vnd' in content_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f98132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar nombres de archivos extraídos de URLs\n",
    "# The cleaned file_name is returned, making it suitable for use as a filename.\n",
    "# THIS FUNCTION IS A COMPONENT FOR download_file() Function\n",
    "\n",
    "# Function to clean filenames\n",
    "def clean_filename(href):\n",
    "    # Extract the file name from the URL\n",
    "    file_name = unquote(os.path.basename(urlparse(href).path))\n",
    "    # Remove invalid characters for filenames using a regex\n",
    "    file_name = re.sub(r'[<>:\"/\\\\|?*]', '_', file_name)\n",
    "    # Fallback if the URL doesn't have a file name\n",
    "    if not file_name:\n",
    "        file_name = \"downloaded_file\"\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a516e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener enlaces de documentos desde un sitio web, navegando recursivamente por enlaces internos.\n",
    "# THIS FUNCTION IS A COMPONEN FOR scrape_and_store_links() and scrape_documents_from_website() FUNCTIONS\n",
    "\n",
    "# Recursively fetch document links from a website, traversing internal links.\n",
    "def get_document_links(url, base_url, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    doc_links = []\n",
    "\n",
    "    # Ensure we do not revisit the same page more than once\n",
    "    if url in visited:\n",
    "        return doc_links\n",
    "    visited.add(url)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.write(f\"Processing: {url}\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Skip binary files (like PDFs, DOCs) to avoid trying to parse them\n",
    "        if is_binary_content(response):\n",
    "            sys.stdout.write(f\"Skipping binary file: {url}\\r\")\n",
    "            sys.stdout.flush()\n",
    "            return doc_links\n",
    "\n",
    "        # Parse HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all document links on the page (pdf, doc, docx)\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith(('tel:', 'mailto:')):\n",
    "                continue\n",
    "            if href.endswith(('pdf', 'doc', 'docx','doc')):\n",
    "                full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "                # Append both the parent page (url) and the document link (full_url)\n",
    "                doc_links.append([url, (link, full_url)])  # Return parent page and document link as 2D array\n",
    "\n",
    "        # Now, find all internal links to recursively navigate\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            full_url = href if href.startswith('http') else urljoin(url, href)\n",
    "            if is_internal_link(full_url, base_url) and full_url not in visited:\n",
    "                doc_links.extend(get_document_links(full_url, base_url, visited))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError fetching {url}: {e}\")\n",
    "\n",
    "    return doc_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fdb41",
   "metadata": {},
   "source": [
    "Purpose\n",
    "The function get_document_links() returns a list of document links found on the site, with each link paired with its originating page. This approach is useful for collecting downloadable files from an entire website, starting from a base URL and exploring all reachable internal pages.\n",
    "\n",
    "OUTCOME EXAMPLE: \n",
    "\n",
    "doc_links = \n",
    "[\n",
    "    [\n",
    "        'https://example.com/page1', \n",
    "        ('Document 1', 'https://example.com/files/doc1.pdf')\n",
    "    ],\n",
    "    [\n",
    "        'https://example.com/page1', \n",
    "        ('Document 2', 'https://example.com/files/doc2.doc')\n",
    "    ],\n",
    "    [\n",
    "        'https://example.com/page2', \n",
    "        ('Research Paper', 'https://example.com/files/research_paper.pdf')\n",
    "    ],\n",
    "    [\n",
    "        'https://example.com/page3', \n",
    "        ('Report 2023', 'https://example.com/files/report.docx')\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3ad5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir una cadena de texto a formato camelCase\n",
    "# THIS FUNCTION IS A COMPONENT OF scrape_and_store_links() function\n",
    "# camelCase, un estilo común de escritura en programación, especialmente útil para nombres de variables y funciones.\n",
    "# La función convierte texto a camelCase (ej. nombreCompleto), ideal para nombres de variables y claves en JSON, siguiendo convenciones de programación\n",
    "\n",
    "def to_camel_case(text):\n",
    "    # Return empty string if the input is empty or None\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'[_\\-]+', ' ', text)\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return ''\n",
    "    camel_case_text = words[0].lower() + ''.join(word.capitalize() for word in words[1:])\n",
    "    \n",
    "    return camel_case_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b9c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer y almacenar enlaces de documentos en JSON, incluyendo los segmentos de ruta de la página principal\n",
    "# Esta funcion crea el archivo .json \"scraped_links_y-m-d-H-M-S) \n",
    "# Extrae metadata parent_page, tittle, href y file_url\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp =datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    ##timestamp = datetime.now().isoformat()  # Timestamp for saving the file\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in doc_links:  # Extract parent_page and document link from the 2D array\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "            \n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473098c5",
   "metadata": {},
   "source": [
    "La función scrape_and_store_links extrae la siguiente metadata para cada documento encontrado:\n",
    "\n",
    "timestamp: La fecha y hora exacta de la extracción, en formato YYYY-MM-DD_HH-MM-SS. Esto facilita saber cuándo se realizó el scraping.\n",
    "\n",
    "parent_page: Los segmentos de la ruta de la página principal donde se encontró el enlace. Cada segmento de la URL se guarda como un elemento en una lista, usando formato camelCase (ej. [\"seccionPrincipal\", \"subseccion\"]). Si no hay ruta, se usa el dominio.\n",
    "\n",
    "title: El texto del enlace al documento, que generalmente describe el documento o su título. Si está vacío, se establece como \"No title\".\n",
    "\n",
    "href: La URL parcial extraída directamente del atributo href del enlace (<a>) en la página.\n",
    "\n",
    "file_url: La URL completa del documento (PDF, DOC, etc.), que se puede utilizar para descargar el archivo.\n",
    "\n",
    "Este conjunto de metadata organiza los documentos y su procedencia, siendo útil para estructurar y consultar enlaces descargables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58cfe9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to initiate scraping for documents\n",
    "# Función principal para iniciar la extracción de documentos desde un sitio web\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "\n",
    "\n",
    "def scrape_documents_from_website(url):\n",
    "    base_url = url  # The starting URL\n",
    "    visited = set()  # To keep track of visited URLs\n",
    "    doc_links = get_document_links(base_url, base_url, visited)\n",
    "\n",
    "    print(f\"Found {len(doc_links)} document links:\")\n",
    "    for title, link in doc_links:\n",
    "        print(f\"- {title}: {link}\")\n",
    "\n",
    "    return doc_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a66cf",
   "metadata": {},
   "source": [
    "LOGICA DE scrape_documents_from_website(x):\n",
    "\n",
    "Si llamamos a la función con un sitio web como https://example.com, el output en la consola podría verse así:\n",
    "scrape_documents_from_website(\"https://example.com\")\n",
    "\n",
    "OUTPUT IS A LIST OF TUPPLES\n",
    "Each tuple in the list contains two elements: the title of the document and the document’s URL.\n",
    "\n",
    "OUTPUT:\n",
    "Found 3 document links:\n",
    "- Document 1: https://example.com/files/doc1.pdf\n",
    "- Report 2022: https://example.com/files/report2022.doc\n",
    "- Guide: https://example.com/downloads/guide.docx\n",
    "\n",
    "Explicación\n",
    "La función muestra:\n",
    "\n",
    "El total de enlaces de documentos encontrados (en este caso, 3).\n",
    "Una lista donde cada línea representa un documento, mostrando su título (Document 1, Report 2022, Guide) y el enlace directo al archivo (doc1.pdf, report2022.doc, guide.docx).\n",
    "Este formato hace fácil ver qué documentos fueron encontrados y sus URLs exactas, útiles para confirmar que los enlaces fueron correctamente detectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e482b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create subdirectories based on the parent page path segments\n",
    "# Función para crear subdirectorios basados en los segmentos de ruta de la página principal\n",
    "# This function organizes documents into a structured folder system based on the webpage hierarchy, making it easier to manage downloaded files according to their origin.\n",
    "# This function is called and a component for download_file() function\n",
    "\n",
    "\n",
    "def create_directory_structure(parent_page_segments, doc_url):\n",
    "    # Convert the path segments array into a directory path\n",
    "    directory_path = os.path.join(DOWNLOAD_DIR, *parent_page_segments)\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d6dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a document and save it in the appropriate subdirectory\n",
    "# Función para descargar un documento y guardarlo en el subdirectorio correspondiente\n",
    "# Esta funcion se activa y es componente del ultimo bloque de codigo main() function\n",
    "\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "\n",
    "    # Create the directory structure based on Page and Section\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "\n",
    "    # Create the full file path to save the file\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"\\nDownloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to download {doc_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccda004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename=\"scraping_log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logging.info(\"Starting the scraping process...\")\n",
    "\n",
    "def download_file(base_url, doc_url, context_href):\n",
    "    # Use the href to generate a meaningful name\n",
    "    file_name = clean_filename(context_href)\n",
    "    directory_path = create_directory_structure(base_url, doc_url)\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(doc_url, timeout=10)  # Add timeout here\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        logging.info(f\"Downloaded: {file_name} to {directory_path}\")\n",
    "        return file_path\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.warning(f\"Timeout occurred for {doc_url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {doc_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b08be267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main domain (base URL) using the tld library\n",
    "# Extraer el dominio principal (URL base) usando la biblioteca tld\n",
    "# NO ESTAMOS USANDO ESTA FUNCION\n",
    "# NO ES ACTIVADA  \n",
    "\n",
    "def get_base_domain(url):\n",
    "    try:\n",
    "        # Extract the top-level domain (e.g., comunidadandina.org)\n",
    "        return get_fld(url, fix_protocol=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting base domain from {url}: {e}\")\n",
    "        return urlparse(url).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf45a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming 'get_document_links' and 'to_camel_case' functions are defined\n",
    "\n",
    "def scrape_and_store_links(urls):\n",
    "    data = []  # List to hold the scraped links\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    for url in tqdm(urls, desc=\"Processing URLs\"):  # Add tqdm progress bar to URLs loop\n",
    "        print(f\"Processing URL: {url}\")\n",
    "        base_url = url  # This is the parent page (where the link is found)\n",
    "\n",
    "        doc_links = get_document_links(base_url, base_url, visited=set())  # Scrape the links from the base URL\n",
    "\n",
    "        for parent_page, (link, doc_url) in tqdm(doc_links, desc=\"Extracting links\", leave=False):  # Inner progress bar for each URL\n",
    "            # Get the title from the link element (use link text)\n",
    "            title = link.string or link.get_text().strip() or \"No title\"  # Fallback to \"No title\" if empty\n",
    "\n",
    "            # Extract the path segments from the parent page (base_url)\n",
    "            parsed_url = urlparse(parent_page)  # Use the parent_page returned by get_document_links\n",
    "            path_segments = parsed_url.path.strip('/').split('/')\n",
    "\n",
    "            # If path is empty, use domain (netloc) as the folder name\n",
    "            if not path_segments or path_segments == ['']:\n",
    "                path_segments = [parsed_url.netloc]  # Fallback to the domain if the path is empty\n",
    "            else:\n",
    "                # Convert to camelCase and skip empty segments\n",
    "                path_segments = [to_camel_case(segment) for segment in path_segments if segment]\n",
    "\n",
    "            # Append the details of each link to the data list, including the parent page path segments\n",
    "            data.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"parent_page\": path_segments,  # Store the path segments as an array\n",
    "                \"title\": title,\n",
    "                \"href\": link['href'],  # Extract href directly from the <a> tag (the document link)\n",
    "                \"file_url\": doc_url,  # Full URL of the document\n",
    "            })\n",
    "\n",
    "    # Convert the list of links into a DataFrame and save as JSON\n",
    "    df = pd.DataFrame(data)\n",
    "    output_file = f\"scraped_links_{timestamp}.json\"\n",
    "    df.to_json(output_file, orient=\"records\", indent=4)\n",
    "    print(f\"Scraping complete. Links saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cd4d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import win32com.client as win32\n",
    "from docx import Document  # For extracting text from .docx\n",
    "import mammoth\n",
    "import pdfplumber\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Extract text from file based on its extension\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".doc\"):\n",
    "        return extract_text_from_doc(file_path)\n",
    "    else:\n",
    "        logging.error(f\"Unsupported file format for {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .docx files using mammoth and clean Unicode escapes\n",
    "def extract_text_from_docx(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as docx_file:\n",
    "            result = mammoth.extract_raw_text(docx_file)\n",
    "            text = result.value\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the DOCX file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .docx file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .pdf files using pdfplumber and clean Unicode escapes\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text  # Concatenate raw text\n",
    "            return text.encode().decode('unicode_escape') if text else \"No text found in the PDF file\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from PDF file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from .doc files using win32com\n",
    "def extract_text_from_doc(file_path):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        doc = word.Documents.Open(file_path)\n",
    "        text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        return text.encode().decode(\"unicode_escape\", \"replace\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from .doc file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to replace characters in JSON files\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    replacements = {\n",
    "        \"\\u00e1\": \"á\", \"\\u00e9\": \"é\", \"\\u00ed\": \"í\", \"\\u00f3\": \"ó\", \"\\u00fa\": \"ú\",\n",
    "        \"\\u00c1\": \"Á\", \"\\u00c9\": \"É\", \"\\u00cd\": \"Í\", \"\\u00d3\": \"Ó\", \"\\u00da\": \"Ú\",\n",
    "        \"\\u00f1\": \"ñ\", \"\\u00d1\": \"Ñ\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        json_str = json.dumps(json_data)\n",
    "        \n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "\n",
    "        updated_json_data = json.loads(json_str)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file_path_here\"\n",
    "extracted_text = extract_text_from_file(file_path)\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09363a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://www.comunidadandina.org/\n",
      "Processing: tel:+5117106573unidadandina.org/archivo-de-convocatorias/e-la-secretaria-general-can//a/\n",
      "Error fetching tel:+5117106573: No connection adapters were found for 'tel:+5117106573'\n",
      "Processing: mailto:bibliocan@comunidadandina.org\n",
      "Error fetching mailto:bibliocan@comunidadandina.org: No connection adapters were found for 'mailto:bibliocan@comunidadandina.org'\n",
      "Processing: https://www.comunidadandina.org/wp-content/uploads/2023/08/INFORME-FINAL-CAAC-PPT-PERU-2023.pdf.pdf2022.xlsxes.xlsxx\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 0/1 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\316504582.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscrape_and_store_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\3447907396.py\u001b[0m in \u001b[0;36mscrape_and_store_links\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mbase_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m  \u001b[1;31m# This is the parent page (where the link is found)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdoc_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Scrape the links from the base URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparent_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_links\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Extracting links\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Inner progress bar for each URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mfull_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0murljoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_internal_link\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_url\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mdoc_links\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_document_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\854505812.py\u001b[0m in \u001b[0;36mget_document_links\u001b[1;34m(url, base_url, visited)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Skip binary files (like PDFs, DOCs) to avoid trying to parse them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mtls_in_tls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             conn = connection.create_connection(\n\u001b[0m\u001b[0;32m    175\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Nico\\anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# scrape_and_store_link\n",
    "\n",
    "urls = read_urls()\n",
    "scrape_and_store_links(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04aea684",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_documents_from_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12424\\3857271340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdownload_documents_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scraped_links_2024-10-15_20-26-53.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'download_documents_from_json' is not defined"
     ]
    }
   ],
   "source": [
    "download_documents_from_json('scraped_links_2024-10-15_20-26-53.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "724e0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character replacements in JSON file completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def replace_characters_in_json_file(output_file):\n",
    "    # Define the replacements for common problematic characters\n",
    "    replacements = {\n",
    "        r'\\u00e1': 'á', r'\\u00e9': 'é', r'\\u00ed': 'í', r'\\u00f3': 'ó', r'\\u00fa': 'ú',\n",
    "        r'\\u00f1': 'ñ', r'\\u00c1': 'Á', r'\\u00c9': 'É', r'\\u00cd': 'Í', r'\\u00d3': 'Ó',\n",
    "        r'\\u00da': 'Ú', r'\\u00d1': 'Ñ'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Read the JSON file\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Step 2: Convert JSON to string and perform replacements\n",
    "        json_str = json.dumps(json_data)  # Convert JSON data to string\n",
    "        \n",
    "        # Perform replacements using the dictionary\n",
    "        for unicode_seq, char in replacements.items():\n",
    "            json_str = re.sub(unicode_seq, char, json_str)\n",
    "        \n",
    "        # Step 3: Convert the string back to JSON format\n",
    "        updated_json_data = json.loads(json_str)\n",
    "        \n",
    "        # Step 4: Write the updated JSON back to the file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(updated_json_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(\"Character replacements in JSON file completed successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the JSON file: {e}\")\n",
    "\n",
    "# Usage\n",
    "replace_characters_in_json_file('results_with_download_2024-10-17T18-19-41.369908.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función principal que organiza el flujo de scraping, almacenamiento y descarga de documentos\n",
    "def main():\n",
    "    urls = read_urls()  # Read URLs from the text file\n",
    "\n",
    "    # Phase 1: Scrape and store links in JSON\n",
    "    links_json_file = scrape_and_store_links(urls)\n",
    "    #links_json_file = \"scraped_links_2024-10-09.json\"\n",
    "\n",
    "    # Phase 2: Download documents using the stored links\n",
    "    download_documents_from_json(links_json_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
